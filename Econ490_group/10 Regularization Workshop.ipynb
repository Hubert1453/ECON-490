{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workshop - Regularization\n",
    "\n",
    "In this workshop, we are going to:\n",
    "\n",
    "1. Tune an elastic-net regression \n",
    "2. Compare the following models:\n",
    "    1. The null model\n",
    "    2. The tuned elastic-net model\n",
    "    3. The trimmed non-regularized model with standardized features\n",
    "    4. The trimmed non-regularized model with non-standardized features\n",
    "    \n",
    "# Preliminaries\n",
    "\n",
    "- Load any necessary packages and/or functions\n",
    "- Load in and prepare the class data\n",
    "- Create x and y with a label of `pct_d_rgdp`\n",
    "- Create `x_train`, `x_test`, `y_train`, `y_test` with\n",
    "    * training size of two-thirds\n",
    "    * random state of 490\n",
    "- Standardize the features\n",
    "- Add constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn import linear_model as lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at `lm.ElasticNet?` and \n",
    "```\n",
    "fit = sm.OLS(y_train, x_train)\n",
    "fit.fit_regularized?\n",
    "```\n",
    "Determine which coefficients are the same, but named differently.\n",
    "Specifically, $\\alpha$ and the weight on the different constraints (i.e. $||\\beta||_2$ and $||\\beta||_1$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m\n",
       "\u001b[0mlm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mElasticNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0ml1_ratio\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mfit_intercept\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mnormalize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mprecompute\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mcopy_X\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0001\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mwarm_start\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mpositive\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mselection\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'cyclic'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m     \n",
       "Linear regression with combined L1 and L2 priors as regularizer.\n",
       "\n",
       "Minimizes the objective function::\n",
       "\n",
       "        1 / (2 * n_samples) * ||y - Xw||^2_2\n",
       "        + alpha * l1_ratio * ||w||_1\n",
       "        + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\n",
       "\n",
       "If you are interested in controlling the L1 and L2 penalty\n",
       "separately, keep in mind that this is equivalent to::\n",
       "\n",
       "        a * L1 + b * L2\n",
       "\n",
       "where::\n",
       "\n",
       "        alpha = a + b and l1_ratio = a / (a + b)\n",
       "\n",
       "The parameter l1_ratio corresponds to alpha in the glmnet R package while\n",
       "alpha corresponds to the lambda parameter in glmnet. Specifically, l1_ratio\n",
       "= 1 is the lasso penalty. Currently, l1_ratio <= 0.01 is not reliable,\n",
       "unless you supply your own sequence of alpha.\n",
       "\n",
       "Read more in the :ref:`User Guide <elastic_net>`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "alpha : float, default=1.0\n",
       "    Constant that multiplies the penalty terms. Defaults to 1.0.\n",
       "    See the notes for the exact mathematical meaning of this\n",
       "    parameter. ``alpha = 0`` is equivalent to an ordinary least square,\n",
       "    solved by the :class:`LinearRegression` object. For numerical\n",
       "    reasons, using ``alpha = 0`` with the ``Lasso`` object is not advised.\n",
       "    Given this, you should use the :class:`LinearRegression` object.\n",
       "\n",
       "l1_ratio : float, default=0.5\n",
       "    The ElasticNet mixing parameter, with ``0 <= l1_ratio <= 1``. For\n",
       "    ``l1_ratio = 0`` the penalty is an L2 penalty. ``For l1_ratio = 1`` it\n",
       "    is an L1 penalty.  For ``0 < l1_ratio < 1``, the penalty is a\n",
       "    combination of L1 and L2.\n",
       "\n",
       "fit_intercept : bool, default=True\n",
       "    Whether the intercept should be estimated or not. If ``False``, the\n",
       "    data is assumed to be already centered.\n",
       "\n",
       "normalize : bool, default=False\n",
       "    This parameter is ignored when ``fit_intercept`` is set to False.\n",
       "    If True, the regressors X will be normalized before regression by\n",
       "    subtracting the mean and dividing by the l2-norm.\n",
       "    If you wish to standardize, please use\n",
       "    :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
       "    on an estimator with ``normalize=False``.\n",
       "\n",
       "precompute : bool or array-like of shape (n_features, n_features),                 default=False\n",
       "    Whether to use a precomputed Gram matrix to speed up\n",
       "    calculations. The Gram matrix can also be passed as argument.\n",
       "    For sparse input this option is always ``True`` to preserve sparsity.\n",
       "\n",
       "max_iter : int, default=1000\n",
       "    The maximum number of iterations\n",
       "\n",
       "copy_X : bool, default=True\n",
       "    If ``True``, X will be copied; else, it may be overwritten.\n",
       "\n",
       "tol : float, default=1e-4\n",
       "    The tolerance for the optimization: if the updates are\n",
       "    smaller than ``tol``, the optimization code checks the\n",
       "    dual gap for optimality and continues until it is smaller\n",
       "    than ``tol``.\n",
       "\n",
       "warm_start : bool, default=False\n",
       "    When set to ``True``, reuse the solution of the previous call to fit as\n",
       "    initialization, otherwise, just erase the previous solution.\n",
       "    See :term:`the Glossary <warm_start>`.\n",
       "\n",
       "positive : bool, default=False\n",
       "    When set to ``True``, forces the coefficients to be positive.\n",
       "\n",
       "random_state : int, RandomState instance, default=None\n",
       "    The seed of the pseudo random number generator that selects a random\n",
       "    feature to update. Used when ``selection`` == 'random'.\n",
       "    Pass an int for reproducible output across multiple function calls.\n",
       "    See :term:`Glossary <random_state>`.\n",
       "\n",
       "selection : {'cyclic', 'random'}, default='cyclic'\n",
       "    If set to 'random', a random coefficient is updated every iteration\n",
       "    rather than looping over features sequentially by default. This\n",
       "    (setting to 'random') often leads to significantly faster convergence\n",
       "    especially when tol is higher than 1e-4.\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "coef_ : ndarray of shape (n_features,) or (n_targets, n_features)\n",
       "    parameter vector (w in the cost function formula)\n",
       "\n",
       "sparse_coef_ : sparse matrix of shape (n_features, 1) or             (n_targets, n_features)\n",
       "    ``sparse_coef_`` is a readonly property derived from ``coef_``\n",
       "\n",
       "intercept_ : float or ndarray of shape (n_targets,)\n",
       "    independent term in decision function.\n",
       "\n",
       "n_iter_ : list of int\n",
       "    number of iterations run by the coordinate descent solver to reach\n",
       "    the specified tolerance.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> from sklearn.linear_model import ElasticNet\n",
       ">>> from sklearn.datasets import make_regression\n",
       "\n",
       ">>> X, y = make_regression(n_features=2, random_state=0)\n",
       ">>> regr = ElasticNet(random_state=0)\n",
       ">>> regr.fit(X, y)\n",
       "ElasticNet(random_state=0)\n",
       ">>> print(regr.coef_)\n",
       "[18.83816048 64.55968825]\n",
       ">>> print(regr.intercept_)\n",
       "1.451...\n",
       ">>> print(regr.predict([[0, 0]]))\n",
       "[1.451...]\n",
       "\n",
       "\n",
       "Notes\n",
       "-----\n",
       "To avoid unnecessary memory duplication the X argument of the fit method\n",
       "should be directly passed as a Fortran-contiguous numpy array.\n",
       "\n",
       "See also\n",
       "--------\n",
       "ElasticNetCV : Elastic net model with best model selection by\n",
       "    cross-validation.\n",
       "SGDRegressor: implements elastic net regression with incremental training.\n",
       "SGDClassifier: implements logistic regression with elastic net penalty\n",
       "    (``SGDClassifier(loss=\"log\", penalty=\"elasticnet\")``).\n",
       "\u001b[1;31mFile:\u001b[0m           e:\\anaconda\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py\n",
       "\u001b[1;31mType:\u001b[0m           ABCMeta\n",
       "\u001b[1;31mSubclasses:\u001b[0m     Lasso\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lm.ElasticNet?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a 5-fold cross-validation grid search with a random state of 490. \n",
    "Identify the optimally tuned hyperparameters.\n",
    "Use this grid:\n",
    "```\n",
    "param_grid = {'alpha': 10.**np.arange(-5, -1, 1), \n",
    "              'l1_ratio': np.arange(0, 1, 0.1)}\n",
    "```\n",
    "You will get a warning message about convergence.\n",
    "We will discuss it after the workshop.\n",
    "Think about why it occuring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "# Question\n",
    "\n",
    "How many models did we just fit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Using the tuned hyperparameters, fit your elastic net model with `statsmodels`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the selected features refit\n",
    "\n",
    "- the non-regularized model with standardized features\n",
    "- the non-regularized model with non-standardized features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the percent improvement from the null model RMSE to the elastic-net and OLS model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
