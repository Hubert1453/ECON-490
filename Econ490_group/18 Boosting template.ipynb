{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting \n",
    "\n",
    "#### Table of Contents\n",
    "\n",
    "- [Preliminaries](#Preliminaries)\n",
    "- [AdaBoost](#AdaBoost)\n",
    "- [Null Model](#Null-Model)\n",
    "- [Manual Gradient Boosting](#Manual-Gradient-Boosting)\n",
    "- [Gradient Boosting](#Gradient-Boosting)\n",
    "- [Extreme Gradient Boosting](#Extreme-Gradient-Boosting)\n",
    "\n",
    "\n",
    "Today, we shall predict `pct_d_rgdp`.\n",
    "We will fit \n",
    "\n",
    "1. an AdaBoost model\n",
    "2. a manual gradient boosting model\n",
    "3. `skearn`'s gradient boosting model\n",
    "4. and finally `xgboost`'s EXTREME!!!!!!!!!! gradient boosting model\n",
    "\n",
    "\n",
    "We are performing a regression problem.\n",
    "\n",
    "```\n",
    "conda install -c conda-forge xgboost\n",
    "```\n",
    "We have been using RMSE, MSE, and MAE to evaluate the performance of regression problems. \n",
    "Now we are going to use $R^2$ to take advantage of `sklearn`'s `.score()` method.\n",
    "\n",
    "HOWEVER! Let's grab our metric functions from our helper script so I can make a point!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run metrics.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utitlties\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#processing\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "\n",
    "# algorithms\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor, GradientBoostingRegressor\n",
    "import xgboost as xgb\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('C:/Users/hubst/Econ490_group/class_data.pkl')\n",
    "df = df.drop(columns = ['urate_bin', 'GeoName']).join([\n",
    "    pd.get_dummies(df['urate_bin'], drop_first = True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['pct_d_rgdp']\n",
    "x = df.drop(columns = 'pct_d_rgdp')\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, \n",
    "                                                    train_size = 2/3, \n",
    "                                                    random_state = 490)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*****\n",
    "# Null Model\n",
    "[TOP](#Boosting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_null = r2(np.mean(y_train), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_null = rmse(np.mean(y_train), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***************\n",
    "# AdaBoost\n",
    "[TOP](#Boosting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostRegressor(base_estimator=DecisionTreeRegressor(max_depth=1),\n",
       "                  learning_rate=0.5, n_estimators=200)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_ada = AdaBoostRegressor(base_estimator = DecisionTreeRegressor(max_depth = 1),\n",
    "                           n_estimators = 200,\n",
    "                           learning_rate = 0.5)\n",
    "reg_ada.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how we did!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.7789815954703778"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_ada = reg_ada.score(x_test, y_test)\n",
    "r2_ada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What? A negative $R^2$?!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.397708716312868"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse_ada = rmse(reg_ada.predict(x_test), y_test)\n",
    "rmse_ada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.295172646932564"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse_null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-8.104385480267595e-06"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have overfit our training data. \n",
    "Perhaps we should take a look at some good ol' cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 1s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.001, 'n_estimators': 25}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "param_grid = {\n",
    "    'n_estimators': [15, 25, 50, 75],\n",
    "    'learning_rate': 10.**np.arange(-6, -2)\n",
    "}\n",
    "\n",
    "ada_cv = AdaBoostRegressor(base_estimator = DecisionTreeRegressor(max_depth = 1),\n",
    "                           random_state = 490)\n",
    "grid_search = GridSearchCV(ada_cv, param_grid,\n",
    "                          cv = 5,\n",
    "                          scoring = 'r2',\n",
    "                          n_jobs = 2).fit(x_train, y_train)\n",
    "\n",
    "best = grid_search.best_params_\n",
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0006505586486580395"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_ada = AdaBoostRegressor(base_estimator = DecisionTreeRegressor(max_depth = 1),\n",
    "                           random_state = 490,\n",
    "                           n_estimators = best['n_estimators'],\n",
    "                           learning_rate = best['learning_rate'])\n",
    "reg_ada.fit(x_train, y_train)\n",
    "\n",
    "r2_ada = reg_ada.score(x_test, y_test)\n",
    "r2_ada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "25\n",
      "50\n",
      "75\n",
      "100\n",
      "125\n",
      "150\n",
      "175\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 200, 25):\n",
    "    print (i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We explained 1% of the variation in the test data... Yikes..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*****\n",
    "# Manual Gradient Boosting\n",
    "[TOP](#Boosting)\n",
    "\n",
    "Our textbook lays out how to manually fit a gradient descent problem. \n",
    "Since the mid-semester feedback told me most students are not reading the textbook, let's demonstrate how to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.03345984800204671"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg1 = DecisionTreeRegressor(max_depth = 2).fit(x_train, y_train)\n",
    "y_train2 = y_train - reg1.predict(x_train)\n",
    "\n",
    "reg2 = DecisionTreeRegressor(max_depth = 2).fit(x_train, y_train2)\n",
    "y_train3 = y_train - reg2.predict(x_train)\n",
    "\n",
    "reg3 = DecisionTreeRegressor(max_depth = 2).fit(x_train, y_train3)\n",
    "\n",
    "yhat = sum(reg.predict(x_test) for reg in (reg1, reg2, reg3))\n",
    "r2_manual =  r2(yhat, y_test)\n",
    "r2_manual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better than AdaBoost...\n",
    "\n",
    "********\n",
    "# Gradient Boosting\n",
    "[TOP](#Boosting)\n",
    "\n",
    "Fortunately, unlike `AdaBoostRegressor()`, `GradientBoostingRegressor()` has early stopping. \n",
    "\n",
    "This means cross-validation is not necessary!! WOOO!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1          83.4084           11.94s\n",
      "         2          83.0673           11.49s\n",
      "         3          82.7788           11.23s\n",
      "         4          82.4451           10.98s\n",
      "         5          82.1454           10.92s\n",
      "         6          81.9084           10.80s\n",
      "         7          81.7115           10.67s\n",
      "         8          81.4859           10.59s\n",
      "         9          81.2805           10.53s\n",
      "        10          81.1250           10.45s\n",
      "        11          80.9172           10.36s\n",
      "        12          80.7497           10.28s\n",
      "        13          80.6237           10.22s\n",
      "        14          80.4921           10.17s\n",
      "        15          80.3611           10.10s\n",
      "        16          80.2315           10.05s\n",
      "        17          80.1085           10.00s\n",
      "        18          79.9921            9.95s\n",
      "        19          79.8975            9.89s\n",
      "        20          79.7921            9.84s\n",
      "        21          79.7015            9.80s\n",
      "        22          79.6153            9.84s\n",
      "        23          79.5238            9.78s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(max_depth=2, n_estimators=200, n_iter_no_change=4,\n",
       "                          validation_fraction=0.125, verbose=2)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_gb = GradientBoostingRegressor(n_estimators = 200,\n",
    "                                  max_depth = 2,\n",
    "                                  learning_rate = 0.1,\n",
    "                                  validation_fraction = 1/8,\n",
    "                                  n_iter_no_change = 4,\n",
    "                                  verbose = 2)\n",
    "reg_gb.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how we did!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.026609031992107846"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_gb = reg_gb.score(x_test, y_test)\n",
    "r2_gb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relatively speaking, not to shabby.\n",
    "Not super exciting either.\n",
    "\n",
    "*****\n",
    "# Extreme Gradient Boosting\n",
    "[TOP](#Boosting)\n",
    "\n",
    "Now we get to test to see if extreme gradient boosting is all that it is made out to be!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_train, x_train_test, y_train_train, y_train_test = train_test_split(x_train, y_train,\n",
    "                                                                           train_size = 4/5,\n",
    "                                                                           random_state = 490)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:04:37] WARNING: ..\\src\\learner.cc:541: \n",
      "Parameters: { learning } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\tvalidation_0-rmse:8.99019\n",
      "[1]\tvalidation_0-rmse:8.94539\n",
      "[2]\tvalidation_0-rmse:8.92961\n",
      "[3]\tvalidation_0-rmse:8.92726\n",
      "[4]\tvalidation_0-rmse:8.89911\n",
      "[5]\tvalidation_0-rmse:8.91130\n",
      "[6]\tvalidation_0-rmse:8.91488\n",
      "[7]\tvalidation_0-rmse:8.91046\n",
      "[8]\tvalidation_0-rmse:8.92315\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "             importance_type='gain', interaction_constraints='', learning=0.1,\n",
       "             learning_rate=0.300000012, max_delta_step=0, max_depth=2,\n",
       "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "             n_estimators=200, n_jobs=12, num_parallel_tree=1, random_state=490,\n",
       "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "             tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_xgb = xgb.XGBRegressor(n_estimators = 200,\n",
    "                          max_depth = 2,\n",
    "                          learning = 0.1,\n",
    "                          random_state = 490)\n",
    "\n",
    "reg_xgb.fit(x_train_train, y_train_train,\n",
    "           eval_set = [(x_train_test, y_train_test)],\n",
    "           early_stopping_rounds = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.017345876922395753"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_xgb = reg_xgb.score(x_test, y_test)\n",
    "r2_xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, there you have it.\n",
    "\n",
    "Let us sloppily print out these values for a conclusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 Null: -8.104385480267595e-06\n",
      "R2 AdaBoost: 0.0006505586486580395\n",
      "R2 Manual: -0.03345984800204671\n",
      "R2 GradientBoosting: 0.026609031992107846\n",
      "R2 XGBoost: 0.017345876922395753\n"
     ]
    }
   ],
   "source": [
    "print('R2 Null:', r2_null)\n",
    "print('R2 AdaBoost:', r2_ada)\n",
    "print('R2 Manual:', r2_manual)\n",
    "print('R2 GradientBoosting:', r2_gb)\n",
    "print('R2 XGBoost:', r2_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
